{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12924586,"sourceType":"datasetVersion","datasetId":8178389}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch torchvision pillow tqdm numpy matplotlib torchmetrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T16:33:43.012072Z","iopub.execute_input":"2025-09-25T16:33:43.012380Z","iopub.status.idle":"2025-09-25T16:33:46.177671Z","shell.execute_reply.started":"2025-09-25T16:33:43.012344Z","shell.execute_reply":"2025-09-25T16:33:46.176805Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: torchmetrics in /usr/local/lib/python3.11/dist-packages (1.7.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (0.14.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\n\n\n# ---------------------------\n# CBAM (Channel + Spatial)\n# ---------------------------\nclass CBAM(nn.Module):\n    def __init__(self, channels, reduction=16, kernel_size=7):\n        super().__init__()\n        # Channel attention\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n        self.mlp = nn.Sequential(\n            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n        )\n        self.sigmoid_channel = nn.Sigmoid()\n\n        # Spatial attention\n        self.conv_spatial = nn.Conv2d(2, 1, kernel_size, padding=(kernel_size//2), bias=False)\n        self.sigmoid_spatial = nn.Sigmoid()\n\n    def forward(self, x):\n        # Channel\n        avg = self.mlp(self.avg_pool(x))\n        max_ = self.mlp(self.max_pool(x))\n        ch_attn = self.sigmoid_channel(avg + max_)\n        x = x * ch_attn\n\n        # Spatial\n        avg_pool = torch.mean(x, dim=1, keepdim=True)\n        max_pool, _ = torch.max(x, dim=1, keepdim=True)\n        spat = torch.cat([avg_pool, max_pool], dim=1)\n        spat_attn = self.sigmoid_spatial(self.conv_spatial(spat))\n\n        return x * spat_attn\n\n\n# ---------------------------\n# Residual Block (small)\n# ---------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(channels, channels, 3, padding=1, bias=False),\n            nn.BatchNorm2d(channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(channels, channels, 3, padding=1, bias=False),\n            nn.BatchNorm2d(channels),\n        )\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        return self.relu(x + self.conv(x))\n        \n\nclass ResNetCBAMEncoder(nn.Module):\n    def __init__(self, pretrained=True, use_cbam=True):\n        super().__init__()\n        resnet = models.resnet34(pretrained=pretrained)\n        self.conv6 = nn.Conv2d(6, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        nn.init.kaiming_normal_(self.conv6.weight, mode='fan_out', nonlinearity='relu')\n        self.bn1 = resnet.bn1\n        self.relu = resnet.relu\n        self.maxpool = resnet.maxpool\n        self.layer1 = resnet.layer1\n        self.layer2 = resnet.layer2\n        self.layer3 = resnet.layer3\n        self.layer4 = resnet.layer4\n        self.use_cbam = use_cbam\n        if use_cbam:\n            self.cbam2 = CBAM(128)\n            self.cbam3 = CBAM(256)\n            self.cbam4 = CBAM(512)\n\n        # Upsample head\n        self.stego_head = nn.Sequential(\n            nn.Conv2d(512, 256, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),   # 8â†’16\n            nn.Conv2d(256, 128, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),   # 16â†’32\n            nn.Conv2d(128, 64, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),   # 32â†’64\n            nn.Conv2d(64, 32, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),   # 64â†’128\n            nn.Conv2d(32, 16, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),   # 128â†’256 âœ…\n            nn.Conv2d(16, 3, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, cover, secret_half):\n        secret_resized = F.interpolate(secret_half, size=cover.shape[2:], mode='bilinear', align_corners=False)\n        x = torch.cat([cover, secret_resized], dim=1)\n        x = self.conv6(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        if self.use_cbam: x = self.cbam2(x)\n        x = self.layer3(x)\n        if self.use_cbam: x = self.cbam3(x)\n        x = self.layer4(x)\n        if self.use_cbam: x = self.cbam4(x)\n        stego = self.stego_head(x)\n        return stego\n\n\nclass ResNetCBAMDecoder(nn.Module):\n    def __init__(self, use_cbam=True):\n        super().__init__()\n        self.use_cbam = use_cbam\n        self.conv_in = nn.Sequential(\n            nn.Conv2d(3, 64, 3, padding=1),\n            nn.ReLU(inplace=True)\n        )\n        self.res1 = ResidualBlock(64)\n        self.res2 = ResidualBlock(64)\n        self.res3 = ResidualBlock(64)\n        if use_cbam:\n            self.cbam = CBAM(64)\n\n        self.secret_head = nn.Sequential(\n            nn.Conv2d(64, 32, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Upsample(size=(128, 256), mode='bilinear', align_corners=False),\n            nn.Conv2d(32, 16, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(16, 3, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, stego):\n        x = self.conv_in(stego)\n        x = self.res1(x)\n        x = self.res2(x)\n        x = self.res3(x)\n        if self.use_cbam:\n            x = self.cbam(x)\n        secret_half = self.secret_head(x)\n        return secret_half\n\n\n\n# ---------------------------\n# Quick utility: count params\n# ---------------------------\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\n\n    rec_half = decoder(stego)                  # [B,3,128,256]\n    print(\"stego.shape\", stego.shape, \"rec_half.shape\", rec_half.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T16:33:53.628428Z","iopub.execute_input":"2025-09-25T16:33:53.628714Z","iopub.status.idle":"2025-09-25T16:34:00.472981Z","shell.execute_reply.started":"2025-09-25T16:33:53.628679Z","shell.execute_reply":"2025-09-25T16:34:00.472208Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchmetrics.functional import structural_similarity_index_measure as ssim\nfrom tqdm import tqdm\n\n# Optional perceptual loss (VGG-based)\nfrom torchvision.models import vgg16\nclass PerceptualLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        vgg = vgg16(weights=\"IMAGENET1K_V1\").features[:16].eval()  # up to conv3_3\n        for p in vgg.parameters():\n            p.requires_grad = False\n        self.vgg = vgg\n\n    def forward(self, pred, target):\n        return nn.functional.l1_loss(self.vgg(pred), self.vgg(target))\n\n\ndef train(encoder, decoder, dataloader, device,start=0, epochs=20, lr=1e-4,save_dir=\"/kaggle/working\"):\n    # Optimizer\n    optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=lr)\n    encoder.to(device)\n    decoder.to(device)\n    # Losses\n    mse_loss = nn.MSELoss()\n    perc_loss = PerceptualLoss().to(device)\n\n    best_loss = float(\"inf\")\n    best_file = os.path.join(save_dir, \"best_loss.pth\")\n    if os.path.exists(best_file):\n        best_loss = torch.load(best_file)  # reload previous best loss\n        print(f\"Resuming training. Previous best loss = {best_loss:.6f}\")\n\n    for epoch in range(start,epochs):\n        encoder.train()\n        decoder.train()\n\n        total_loss = 0.0\n        total_perc = 0.0\n        total_ssim = 0.0\n\n        loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n\n        for covers, secret in loop:\n            cover1, cover2 = covers\n            cover1, cover2, secret = cover1.to(device), cover2.to(device), secret.to(device)\n\n            # Split secret into halves\n            secret_top = secret[:, :, 0:128, :]\n            secret_bottom = secret[:, :, 128:256, :]\n\n            # Forward pass\n            stego1 = encoder(cover1, secret_top)\n            stego2 = encoder(cover2, secret_bottom)\n\n            rec_top = decoder(stego1)\n            rec_bottom = decoder(stego2)\n            rec_secret = torch.cat([rec_top, rec_bottom], dim=2)               # decoder output\n\n            if stego1.shape[2:] != cover1.shape[2:]:\n                print(\"Miss match\")\n                stego1 = F.interpolate(stego1, size=cover1.shape[2:], mode=\"bilinear\", align_corners=False)\n            if stego2.shape[2:] != cover2.shape[2:]:\n                stego2 = F.interpolate(stego2, size=cover2.shape[2:], mode=\"bilinear\", align_corners=False)\n\n            # Cover imperceptibility\n            loss_cover_mse = mse_loss(stego1, cover1) + mse_loss(stego2, cover2)\n            loss_cover_perc = perc_loss(stego1, cover1) + perc_loss(stego2, cover2)\n\n            # Secret recoverability\n            loss_secret_mse = mse_loss(rec_secret, secret)\n            loss_secret_ssim = 1 - ssim(rec_secret, secret)\n            loss_secret_perc = perc_loss(rec_secret, secret)\n\n            # Hybrid Loss\n            loss_cover = loss_cover_mse + 0.2 * loss_cover_perc\n            loss_secret = loss_secret_mse + 0.5 * loss_secret_ssim + 0.2 * loss_secret_perc\n            loss = loss_cover + 2*loss_secret\n\n\n            # Backward\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # Metrics\n            batch_perc = perc_loss(rec_secret, secret)\n            batch_ssim = ssim(rec_secret, secret)\n\n            total_loss += loss.item()\n            total_perc += batch_perc.item()\n            total_ssim += batch_ssim.item()\n\n            loop.set_postfix(loss=loss.item(),\n                             perc=batch_perc.item(),\n                             ssim=batch_ssim.item())\n\n        # Epoch averages\n        avg_loss = total_loss / len(dataloader)\n        avg_perc = total_perc / len(dataloader)\n        avg_ssim = total_ssim / len(dataloader)\n\n        print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, PERC={avg_perc:.2f}, SSIM={avg_ssim:.4f}\")\n\n        torch.save(encoder.state_dict(), os.path.join(save_dir, f\"encoder_epoch{epoch+1}.pth\"))\n        torch.save(decoder.state_dict(), os.path.join(save_dir, f\"decoder_epoch{epoch+1}.pth\"))\n\n        # Save best model\n        if avg_loss < best_loss:\n            best_loss = avg_loss\n            torch.save(encoder.state_dict(), os.path.join(save_dir, \"encoder_best.pth\"))\n            torch.save(decoder.state_dict(), os.path.join(save_dir, \"decoder_best.pth\"))\n            torch.save(best_loss, best_file)\n            print(\"âœ… Saved best model\")\n\n    print(\"Training finished!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T16:34:07.284529Z","iopub.execute_input":"2025-09-25T16:34:07.285307Z","iopub.status.idle":"2025-09-25T16:34:11.926723Z","shell.execute_reply.started":"2025-09-25T16:34:07.285284Z","shell.execute_reply":"2025-09-25T16:34:11.926155Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import os\nimport random\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport torchvision.transforms as transforms\n\nclass StegoDataset(Dataset):\n    def __init__(self, dataset_dir, image_size=256):\n        self.dataset_dir = dataset_dir\n        self.images = sorted(os.listdir(dataset_dir))\n\n        # image preprocessing\n        self.transform = transforms.Compose([\n            transforms.Resize((image_size, image_size)),\n            transforms.ToTensor()\n        ])\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        # pick secret image\n        secret_path = os.path.join(self.dataset_dir, self.images[idx])\n        secret = Image.open(secret_path).convert(\"RGB\")\n\n        # pick two random cover images (not equal to secret)\n        cover_choices = list(range(len(self.images)))\n        cover_choices.remove(idx)\n        cover1_idx, cover2_idx = random.sample(cover_choices, 2)\n\n        cover1_path = os.path.join(self.dataset_dir, self.images[cover1_idx])\n        cover2_path = os.path.join(self.dataset_dir, self.images[cover2_idx])\n\n        cover1 = Image.open(cover1_path).convert(\"RGB\")\n        cover2 = Image.open(cover2_path).convert(\"RGB\")\n\n        # apply transforms\n        cover1 = self.transform(cover1)\n        cover2 = self.transform(cover2)\n        secret = self.transform(secret)\n\n        return (cover1, cover2), secret\n\n\ndef get_dataloader(dataset_dir, batch_size=8, image_size=256, shuffle=True):\n    dataset = StegoDataset(dataset_dir, image_size=image_size)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n    return dataloader\n\ndef get_half_dataloader(dataset_dir, batch_size=8, image_size=256, shuffle=True, first_half=True):\n    \"\"\"\n    Returns a dataloader for either the first half or second half of the dataset.\n    \n    Args:\n        dataset_dir (str): Path to dataset\n        batch_size (int): Batch size\n        image_size (int): Image size\n        shuffle (bool): Whether to shuffle\n        first_half (bool): If True, use first half; else use second half\n    \n    Returns:\n        DataLoader\n    \"\"\"\n    dataset = StegoDataset(dataset_dir, image_size=image_size)\n    half_len = len(dataset) // 25\n\n    if first_half:\n        indices = list(range(half_len))\n    else:\n        indices = list(range(half_len, len(dataset)))\n\n    subset = Subset(dataset, indices)\n    dataloader = DataLoader(subset, batch_size=batch_size, shuffle=shuffle)\n    return dataloader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T16:34:14.906730Z","iopub.execute_input":"2025-09-25T16:34:14.907192Z","iopub.status.idle":"2025-09-25T16:34:14.916818Z","shell.execute_reply.started":"2025-09-25T16:34:14.907171Z","shell.execute_reply":"2025-09-25T16:34:14.916059Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"dataset_path = \"/kaggle/input/pimagenet/AllData\"   # your dataset folder path\ntrain_loader = get_half_dataloader(dataset_path, batch_size=3, image_size=256)\nencoder_model=ResNetCBAMEncoder()\ndecoder_model=ResNetCBAMDecoder()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T16:34:18.965642Z","iopub.execute_input":"2025-09-25T16:34:18.966206Z","iopub.status.idle":"2025-09-25T16:34:25.238719Z","shell.execute_reply.started":"2025-09-25T16:34:18.966183Z","shell.execute_reply":"2025-09-25T16:34:25.237997Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 83.3M/83.3M [00:00<00:00, 212MB/s]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"start=10","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T16:34:32.609436Z","iopub.execute_input":"2025-09-25T16:34:32.610206Z","iopub.status.idle":"2025-09-25T16:34:32.613510Z","shell.execute_reply.started":"2025-09-25T16:34:32.610179Z","shell.execute_reply":"2025-09-25T16:34:32.612814Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"encoder_model.load_state_dict(torch.load(f\"/kaggle/working/encoder_epoch{start}.pth\"))\ndecoder_model.load_state_dict(torch.load(f\"/kaggle/working/decoder_epoch{start}.pth\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T16:34:35.765653Z","iopub.execute_input":"2025-09-25T16:34:35.766270Z","iopub.status.idle":"2025-09-25T16:34:36.109785Z","shell.execute_reply.started":"2025-09-25T16:34:35.766246Z","shell.execute_reply":"2025-09-25T16:34:36.109060Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"device=device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T16:34:45.227098Z","iopub.execute_input":"2025-09-25T16:34:45.227911Z","iopub.status.idle":"2025-09-25T16:34:45.232014Z","shell.execute_reply.started":"2025-09-25T16:34:45.227883Z","shell.execute_reply":"2025-09-25T16:34:45.231227Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"train(encoder_model, decoder_model, train_loader, device=device, start=start, epochs=100,save_dir=\"/kaggle/working\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T16:34:48.301461Z","iopub.execute_input":"2025-09-25T16:34:48.302025Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 528M/528M [00:02<00:00, 242MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Resuming training. Previous best loss = 0.840534\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/100:   0%|          | 0/7198 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torchmetrics/utilities/prints.py:70: FutureWarning: Importing `spectral_angle_mapper` from `torchmetrics.functional` was deprecated and will be removed in 2.0. Import `spectral_angle_mapper` from `torchmetrics.image` instead.\n  _future_warning(\nEpoch 11/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7198/7198 [44:23<00:00,  2.70it/s, loss=1.09, perc=0.678, ssim=0.483] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Loss=0.8301, PERC=0.46, SSIM=0.6328\nâœ… Saved best model\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/100:   3%|â–Ž         | 240/7198 [01:24<41:00,  2.83it/s, loss=0.697, perc=0.383, ssim=0.691]","output_type":"stream"}],"execution_count":null}]}